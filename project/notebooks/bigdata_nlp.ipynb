{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bigdata-nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2iHczldOqHW",
        "outputId": "a085b23a-e680-4e62-8725-1e9fa9c68c5c"
      },
      "source": [
        "import os\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip3 install --ignore-installed pyspark==2.4.5\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip3 install --ignore-installed spark-nlp==2.4.5\n",
        "\n",
        "# Install nltk\n",
        "! pip3 install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_275\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)\n",
            "Collecting pyspark==2.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 65kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 39.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257928 sha256=03edaa7b261723adc428cf83e8964cac9bbdcd2d0d7b5a9ff29550913d90f530\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n",
            "Collecting spark-nlp==2.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/46/5c5a2bda407f693126386da5378f132e5e163fa6dfa46109548270348786/spark_nlp-2.4.5-py2.py3-none-any.whl (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 4.1MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.4.5\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-hHvO2FPC_F"
      },
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z37j8oDFPXv1"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "data_path = './Part9.csv'\n",
        "data = spark.read.csv(data_path, header=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MdBAxuYQIyq",
        "outputId": "765a4c7b-c028-4a88-c041-c9dddce92aa7"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_c0',\n",
              " 'Unnamed: 0',\n",
              " 'date',\n",
              " 'year',\n",
              " 'month',\n",
              " 'day',\n",
              " 'author',\n",
              " 'title',\n",
              " 'article',\n",
              " 'url',\n",
              " 'section',\n",
              " 'publication']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpvHODAbQKUe"
      },
      "source": [
        "text_col = 'article'\n",
        "article_text = data.select(text_col).filter(F.col(text_col).isNotNull())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYaG4zaYSYlw",
        "outputId": "434f48b5-4fc5-4a01-b521-682fad4231a4"
      },
      "source": [
        "article_text.limit(5).show(truncate=90)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                                   article|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|big ticket The biggest sales in October, though, were once again at 220 Central Park So...|\n",
            "|George P. Kent testified that he saw President Trump’s demands for Ukraine to “initiate...|\n",
            "|The top American diplomat in Ukraine, who is to be the first witness in public House im...|\n",
            "|“The Minutes,” which aims to capture fractious American politics by focusing on a City ...|\n",
            "|(Want to get this briefing by email? Here’s the sign-up.) Good evening. Here’s the late...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqhoPdJbSaqK"
      },
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol(text_col) \\\n",
        "     .setOutputCol('document')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzEdhoxKSdLN"
      },
      "source": [
        "from sparknlp.annotator import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['document']) \\\n",
        "     .setOutputCol('tokenized')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YpCnL49SfW9"
      },
      "source": [
        "from sparknlp.annotator import Normalizer\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "     .setInputCols(['tokenized']) \\\n",
        "     .setOutputCol('normalized') \\\n",
        "     .setLowercase(True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR2CJwt7Sinw",
        "outputId": "dbd81171-68f7-486e-ee21-895e45cf216a"
      },
      "source": [
        "from sparknlp.annotator import LemmatizerModel\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "     .setInputCols(['normalized']) \\\n",
        "     .setOutputCol('lemmatized')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV8rKpQnSk5E",
        "outputId": "291b47d9-294d-47d8-b8d5-536fb6619e5b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "eng_stopwords = stopwords.words('english')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GKoPFeoSoLp"
      },
      "source": [
        "from sparknlp.annotator import StopWordsCleaner\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "     .setInputCols(['lemmatized']) \\\n",
        "     .setOutputCol('unigrams') \\\n",
        "     .setStopWords(eng_stopwords)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy5_PpZ6SqgX"
      },
      "source": [
        "from sparknlp.annotator import NGramGenerator\n",
        "\n",
        "ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['lemmatized']) \\\n",
        "    .setOutputCol('ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFWrec-FSs6Y",
        "outputId": "36d3c881-d551-40d9-cb8d-5d5f92eb5d0a"
      },
      "source": [
        "from sparknlp.annotator import PerceptronModel\n",
        "\n",
        "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
        "    .setInputCols(['document', 'lemmatized']) \\\n",
        "    .setOutputCol('pos')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeZHc17-SvXw"
      },
      "source": [
        "from sparknlp.base import Finisher\n",
        "\n",
        "finisher = Finisher() \\\n",
        "     .setInputCols(['unigrams', 'ngrams', 'pos']) \\"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI_gBxl1TXyh"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline() \\\n",
        "     .setStages([documentAssembler,                  \n",
        "                 tokenizer,\n",
        "                 normalizer,                  \n",
        "                 lemmatizer,                  \n",
        "                 stopwords_cleaner, \n",
        "                 pos_tagger,\n",
        "                 ngrammer,  \n",
        "                 finisher])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWsncDRvTYYq"
      },
      "source": [
        "processed_review = pipeline.fit(article_text).transform(article_text)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a73urVoPTbBl",
        "outputId": "470e2f2a-5da5-4c98-9d67-ba988ab980b0"
      },
      "source": [
        "processed_review.limit(5).show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|             article|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|big ticket The bi...|[big, ticket, big...|[big, ticket, the...|[JJ, NN, DT, JJ, ...|\n",
            "|George P. Kent te...|[george, p, kent,...|[george, p, kent,...|[NNP, NN, NN, NN,...|\n",
            "|The top American ...|[top, american, d...|[the, top, americ...|[DT, JJ, JJ, NN, ...|\n",
            "|“The Minutes,” wh...|[minute, aim, cap...|[the, minute, whi...|[DT, NN, WDT, NN,...|\n",
            "|(Want to get this...|[want, get, brief...|[want, to, get, t...|[VB, TO, VB, DT, ...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNQsFe30TfHU"
      },
      "source": [
        "from pyspark.sql import types as T\n",
        "\n",
        "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
        "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-NTKB-rT61L"
      },
      "source": [
        "pos_documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol('finished_pos') \\\n",
        "     .setOutputCol('pos_document')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzXs0W1BT9BC"
      },
      "source": [
        "pos_tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['pos_document']) \\\n",
        "     .setOutputCol('pos')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fDEhzwLT_YB"
      },
      "source": [
        "pos_ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['pos']) \\\n",
        "    .setOutputCol('pos_ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPcnerT9UBuJ"
      },
      "source": [
        "pos_finisher = Finisher() \\\n",
        "     .setInputCols(['pos', 'pos_ngrams']) \\"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELd7_PdhUDo5"
      },
      "source": [
        "pos_pipeline = Pipeline() \\\n",
        "     .setStages([pos_documentAssembler,                  \n",
        "                 pos_tokenizer,\n",
        "                 pos_ngrammer,  \n",
        "                 pos_finisher])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My0YxZhlUGPh"
      },
      "source": [
        "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_MLNEO7UIrw",
        "outputId": "601c162f-8594-4522-a51f-7d0f970087a3"
      },
      "source": [
        "processed_review.columns"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['article',\n",
              " 'finished_unigrams',\n",
              " 'finished_ngrams',\n",
              " 'finished_pos',\n",
              " 'finished_pos_ngrams']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6Jcz7znUKZx",
        "outputId": "53ac75c3-e6c0-4a40-aa6c-6b3637568b43"
      },
      "source": [
        "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|     finished_ngrams| finished_pos_ngrams|\n",
            "+--------------------+--------------------+\n",
            "|[big, ticket, the...|[JJ, NN, DT, JJ, ...|\n",
            "|[george, p, kent,...|[NNP, NN, NN, NN,...|\n",
            "|[the, top, americ...|[DT, JJ, JJ, NN, ...|\n",
            "|[the, minute, whi...|[DT, NN, WDT, NN,...|\n",
            "|[want, to, get, t...|[VB, TO, VB, DT, ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdZdmml-UMpI"
      },
      "source": [
        "def filter_pos(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
        "\n",
        "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVoR2rI1UQMY"
      },
      "source": [
        "processed_review = processed_review.withColumn('filtered_unigrams',\n",
        "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
        "                                                              F.col('finished_pos')))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE4uE6acUSum",
        "outputId": "023e5154-a09c-4dad-831b-8ad7565348a5"
      },
      "source": [
        "processed_review.select('filtered_unigrams').limit(5).show(truncate=90)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                         filtered_unigrams|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[big, ticket, sales, october, south, million, fashion, hilfiger, wife, duplex, atop, la...|\n",
            "|[p, kent, testify, trump, demand, ukraine, initiate, motivate, corrupt, senior, state, ...|\n",
            "|[american, diplomat, ukraine, witness, house, hearings, investigator, rudolph, giuliani...|\n",
            "|[aim, fractious, politics, focus, city, council, also, hammer, jessie, mueller, city, c...|\n",
            "|[want, brief, signup, even, soar, hope, country, agree, roll, tariff, final, agreement,...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNSPUPwEUU0O"
      },
      "source": [
        "def filter_pos_combs(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if (len(pos.split('_')) == 2 and \\\n",
        "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
        "            or (len(pos.split('_')) == 3 and \\\n",
        "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
        "    \n",
        "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9xDyENjUXud"
      },
      "source": [
        "processed_review = processed_review.withColumn('filtered_ngrams',\n",
        "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
        "                                                                    F.col('finished_pos_ngrams')))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0spGjEAUaKe",
        "outputId": "16e1f466-43a9-4d5d-fd8b-f0b7aee269b9"
      },
      "source": [
        "processed_review.select('filtered_ngrams').limit(5).show(truncate=90)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                           filtered_ngrams|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[big_ticket, big_sales, central_park, fashion_designer, designer_tommy, tommy_hilfiger,...|\n",
            "|[p_kent, kent_testify, see_president, president_trump, trump_demand, motivate_prosecuti...|\n",
            "|[top_american, american_diplomat, first_witness, public_house, house_impeachment, impea...|\n",
            "|[capture_fractious, fractious_american, american_politics, city_council, council_meet, ...|\n",
            "|[signup_good, late_stock, stock_soar, uschina_trade, trade_deal, be_part, final_agreeme...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lc3T1RqUb0F"
      },
      "source": [
        "from pyspark.sql.functions import concat\n",
        "\n",
        "processed_review = processed_review.withColumn('final', \n",
        "                                               concat(F.col('filtered_unigrams'), \n",
        "                                                      F.col('filtered_ngrams')))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoOcpBdXUerV",
        "outputId": "72f9ab08-06dd-4349-fa67-76874f25cf79"
      },
      "source": [
        "processed_review.select('final').limit(5).show(truncate=90)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                                     final|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[big, ticket, sales, october, south, million, fashion, hilfiger, wife, duplex, atop, la...|\n",
            "|[p, kent, testify, trump, demand, ukraine, initiate, motivate, corrupt, senior, state, ...|\n",
            "|[american, diplomat, ukraine, witness, house, hearings, investigator, rudolph, giuliani...|\n",
            "|[aim, fractious, politics, focus, city, council, also, hammer, jessie, mueller, city, c...|\n",
            "|[want, brief, signup, even, soar, hope, country, agree, roll, tariff, final, agreement,...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvpdymtHUhId",
        "outputId": "5cc4bc83-766d-43e7-c0a7-28c5c98522cd"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
        "tf_model = tfizer.fit(processed_review)\n",
        "tf_result = tf_model.transform(processed_review)\n",
        "\n",
        "end = time.time()\n",
        "print('Train time: ', end - start)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train time:  679.9560749530792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrkG6qTlUlPE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5337c65e-881d-4c57-ba47-f242e304d9fb"
      },
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
        "idf_model = idfizer.fit(tf_result)\n",
        "tfidf_result = idf_model.transform(tf_result)\n",
        "\n",
        "end = time.time()\n",
        "print('Train time: ', end - start)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train time:  669.8000109195709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpYGkzMqnUdL",
        "outputId": "f7dca472-9afa-44dc-fce6-f7ae2020face"
      },
      "source": [
        "tfidf_result.show(1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|             article|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|   filtered_unigrams|     filtered_ngrams|               final|         tf_features|     tf_idf_features|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|big ticket The bi...|[big, ticket, big...|[big, ticket, the...|[JJ, NN, DT, JJ, ...|[JJ, NN, DT, JJ, ...|[big, ticket, sal...|[big_ticket, big_...|[big, ticket, sal...|(262144,[1,4,6,8,...|(262144,[1,4,6,8,...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuQH1gu5UnGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df4ba39-a4a1-4573-aa70-513db7088950"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "num_topics = 10\n",
        "max_iter = 10\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features')\n",
        "lda_model = lda.fit(tfidf_result)\n",
        "\n",
        "end = time.time()\n",
        "print('Train time: ', end - start)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train time:  7726.943974733353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRNWluL7UqNp"
      },
      "source": [
        "vocab = tf_model.vocabulary\n",
        "\n",
        "def get_words(token_list):\n",
        "     return [vocab[token_id] for token_id in token_list]\n",
        "       \n",
        "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWPXDUoKUtCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd463e3-648b-4b8d-961d-0794859b0726"
      },
      "source": [
        "num_top_words = 20\n",
        "\n",
        "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
        "topics.select('topic', 'topicWords').show(truncate=90)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+------------------------------------------------------------------------------------------+\n",
            "|topic|                                                                                topicWords|\n",
            "+-----+------------------------------------------------------------------------------------------+\n",
            "|    0|[yard, goal, score, game, touchdown, ornament, pass, second, tide, third_period, period...|\n",
            "|    1|[mr, trump, new, year, president, like, make, say, would, company, get, think, wine, wa...|\n",
            "|    2|[world_cup, hun_sen, q, prim, sokha, manuka, m_share, rapinoe, ipo_m, m_ipo_m, share_pr...|\n",
            "|    3|[trade, oil, deal, market, prime_day, say, price, index, million, china, year, rise, cr...|\n",
            "|    4|[sexy_man, man_alive, taliban, million, legend, sexy, company_coverage, source_text, it...|\n",
            "|    5|[billion, million, revenue, quarter, profit, big_machine, net, year_early, company, swi...|\n",
            "|    6|[la, epstein, los, patient, que, con, una, sex, garrett, de, crime, police, un, murder,...|\n",
            "|    7|[de, que, en, la, el, point, una, los, del, score, para, lead, thomson_reuter, game, se...|\n",
            "|    8|[people, say, mr, trump, state, one, government, go, time, like, tell, woman, child, co...|\n",
            "|    9|[bank, hong, kong, central_bank, loan, rate, protester, inflation, central, say, intere...|\n",
            "+-----+------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrJ7S2YDUvdA"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}